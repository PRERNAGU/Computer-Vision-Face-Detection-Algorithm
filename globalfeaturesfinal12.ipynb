{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['78', 'C:\\\\Users\\\\Prerna Prakash Gupta\\\\Desktop\\\\faces1\\\\modt\\\\10278.JPG']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the entire training data \n",
    "#consisting of faces of images, faces extracted out of video frames, faces out of group photos; and augmented images\n",
    "#entire training data stored as a list with ID; in the format [[ID; image path]] in list girlb\n",
    "#import the entire training data \n",
    "#consisting of faces of images, faces extracted out of video frames, faces out of group photos; and augmented images\n",
    "#entire training data stored as a list with ID; in the format [[ID; image path]] in list girlb\n",
    "import os, sys\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np # linear algebra\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from skimage import data\n",
    "from skimage import transform as tf\n",
    "from skimage.feature import (match_descriptors, corner_harris,\n",
    "                             corner_peaks, ORB, plot_matches)\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np \n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import glob\n",
    "import numpy as np \n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np # linear algebra\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = (r'C:\\Users\\Prerna Prakash Gupta\\Desktop\\faces1\\jpeg1')\n",
    "girla=[]\n",
    "for root, dirs, files in os.walk(file_path):    \n",
    "    for name in files: \n",
    "        v=os.path.join(root, name)\n",
    "        girla.append([root[-2:], v])\n",
    "#print(len(s))\n",
    "girla\n",
    "\n",
    "\n",
    "r=[]\n",
    "for i in range(len(girlb)):\n",
    "    r.append(girlb[i][0])\n",
    "r=pd.Series(r)  \n",
    "q=r.unique()\n",
    "girlb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage import transform as tf\n",
    "from skimage.feature import (match_descriptors, corner_harris, corner_peaks, ORB, plot_matches)\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import mahotas\n",
    "import cv2\n",
    "import os\n",
    "import h5py\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import mahotas\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for feature extraction, create three functions, consisting of three feature extractors, \n",
    "\n",
    "\n",
    "# feature-descriptor-1: Hu Moments\n",
    "def fd_hu_moments(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return feature\n",
    "\n",
    "# feature-descriptor-2: Haralick Texture\n",
    "def fd_haralick(image):\n",
    "    # convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # compute the haralick texture feature vector\n",
    "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
    "    # return the result\n",
    "    return haralick\n",
    "\n",
    "\n",
    "# feature-descriptor-3: Color Histogram\n",
    "def fd_histogram(image, mask=None):\n",
    "    # convert the image to HSV color-space\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # compute the color histogram\n",
    "    hist  = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    # normalize the histogram\n",
    "    cv2.normalize(hist, hist)\n",
    "    # return the histogram\n",
    "    return hist.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalfeature=[]\n",
    "labels=[]\n",
    "#loop over training image paths\n",
    "for i in range(len(girlb)):\n",
    "    #read image as a numpy array\n",
    "    image=imread(girlb[i][1])\n",
    "    image = cv2.resize(image, (128, 64))\n",
    "    #gloabal feature extraction\n",
    "    fv_hu_moments = fd_hu_moments(image)\n",
    "    fv_haralick   = fd_haralick(image)\n",
    "    fv_histogram  = fd_histogram(image)\n",
    "    \n",
    "    # Concatenate global features\n",
    "    global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
    "    \n",
    "    # update the list of labels and feature vectors (store all image labels and features in respective lists)\n",
    "    labels.append(girlb[i][0])\n",
    "    globalfeature.append(global_feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4616, 4616)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(globalfeature), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#the independent variable x is global features and dependent variable y is ID name \n",
    "# we scale both x and y and stack them as numpy arrays in a form readable by ski-kit learn models\n",
    "\n",
    "\n",
    "\n",
    "unscaled_x = np.vstack(globalfeature).astype(np.float64)\n",
    "scaler = StandardScaler().fit(unscaled_x)\n",
    "x = scaler.transform(unscaled_x)\n",
    "y = np.hstack(labels)\n",
    "\n",
    "#create a stratified tarining and test split of the data, with 30% test data to evaluate the model\n",
    "  \n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "                        x, np.ravel(y), stratify=y,\n",
    "                test_size = 0.30, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 3231\n",
      "number of test examples = 1385\n",
      "X_train shape: (3231, 532)\n",
      "X_test shape: (1385, 532)\n"
     ]
    }
   ],
   "source": [
    "print (\"number of training examples =\", X_train.shape[0])\n",
    "print (\"number of test examples =\", X_test.shape[0])\n",
    "print (\"X_train shape:\", X_train.shape)\n",
    "print (\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of outputs :  48\n",
      "Output classes :  ['01' '02' '03' '04' '05' '06' '07' '08' '09' '10' '11' '12' '13' '14'\n",
      " '15' '16' '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28'\n",
      " '29' '30' '31' '32' '33' '34' '36' '38' '40' '42' '44' '46' '48' '50'\n",
      " '52' '54' '56' '58' '60' '78']\n"
     ]
    }
   ],
   "source": [
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(y_train)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for data1: 0.9439767251568323\n",
      "Best C: 1\n",
      "Best Kernel: linear\n",
      "Best Gamma: scale\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "#Perform hyper-parameter grid search to obtain the most optimal SVM model\n",
    "import numpy as np # linear algebra\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import random\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "\n",
    "\n",
    "#Perform hyper-parameter grid search to obtain the most optimal SVM model\n",
    "parameter_candidates = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "]\n",
    "# Create a classifier object with the classifier and parameter candidates\n",
    "clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=-1)\n",
    "\n",
    "# Train the classifier on data1's feature and target data\n",
    "clf.fit(X_train, y_train)   \n",
    "print('Best score for data1:', clf.best_score_) \n",
    "print('Best C:',clf.best_estimator_.C) \n",
    "print('Best Kernel:',clf.best_estimator_.kernel)\n",
    "print('Best Gamma:',clf.best_estimator_.gamma)\n",
    "\n",
    "print(clf.best_params_) \n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning \n",
    "print(clf.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          01       1.00      0.91      0.95        33\n",
      "          02       0.97      1.00      0.98        29\n",
      "          03       0.89      0.94      0.91        33\n",
      "          04       0.98      0.98      0.98        44\n",
      "          05       1.00      0.75      0.86         4\n",
      "          06       1.00      0.97      0.99        39\n",
      "          07       0.67      0.40      0.50         5\n",
      "          08       0.96      0.96      0.96        47\n",
      "          09       0.93      0.89      0.91        28\n",
      "          10       0.94      0.98      0.96        48\n",
      "          11       1.00      1.00      1.00         9\n",
      "          12       0.96      0.96      0.96        47\n",
      "          13       0.89      0.95      0.92        43\n",
      "          14       1.00      0.93      0.97        45\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       1.00      0.87      0.93        15\n",
      "          17       1.00      1.00      1.00        36\n",
      "          18       0.95      0.93      0.94        41\n",
      "          19       1.00      0.89      0.94        19\n",
      "          20       0.95      0.97      0.96        37\n",
      "          21       0.94      0.96      0.95        48\n",
      "          22       0.96      0.94      0.95        47\n",
      "          23       0.94      1.00      0.97        17\n",
      "          24       0.94      0.94      0.94        31\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       0.98      0.96      0.97        49\n",
      "          27       0.98      0.94      0.96        51\n",
      "          28       0.71      0.83      0.77         6\n",
      "          29       0.75      0.82      0.78        11\n",
      "          30       0.83      1.00      0.91         5\n",
      "          31       0.96      0.96      0.96        51\n",
      "          32       1.00      0.90      0.95        21\n",
      "          33       0.96      0.98      0.97        47\n",
      "          34       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00        24\n",
      "          38       1.00      0.91      0.95        11\n",
      "          40       0.78      0.70      0.74        10\n",
      "          42       0.94      0.98      0.96        50\n",
      "          44       0.95      0.95      0.95        22\n",
      "          46       1.00      0.97      0.99        40\n",
      "          48       0.88      0.96      0.92        47\n",
      "          50       0.98      0.95      0.97        44\n",
      "          52       0.88      0.95      0.91        38\n",
      "          54       0.85      0.73      0.79        15\n",
      "          56       0.56      1.00      0.71         5\n",
      "          58       0.83      1.00      0.91         5\n",
      "          60       0.98      0.96      0.97        47\n",
      "          78       0.97      0.97      0.97        34\n",
      "\n",
      "    accuracy                           0.95      1385\n",
      "   macro avg       0.93      0.93      0.93      1385\n",
      "weighted avg       0.95      0.95      0.95      1385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Post hyper-parameter optimization; we run the most optimum SVM model post grid search\n",
    "\n",
    "#load libraries\n",
    "import numpy as np # linear algebra\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import random\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "\n",
    "#Post hyper-parameter optimization; we run the most optimum SVM model post grid search\n",
    "clf=svm.SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
    "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "#fit the model on training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#evaluate the trained model on test-data\n",
    "predictions = clf.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948014440433213\n"
     ]
    }
   ],
   "source": [
    "#create a folder for saving models from JOBLIB\n",
    "import joblib\n",
    "os.chdir(r'C:\\Users\\Prerna Prakash Gupta\\Desktop\\savedmodel')\n",
    "filename = 'SupportvectormachinemodelGL.sav'\n",
    "joblib.dump(clf, filename)\n",
    "# load the model from disk\n",
    "loaded_model = joblib.load('SupportvectormachinemodelGL.sav')\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "#we see 95% accuracy on test data\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best leaf_size: 1\n",
      "Best n_neighbors: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          01       1.00      0.85      0.92        33\n",
      "          02       0.93      0.97      0.95        29\n",
      "          03       0.78      0.94      0.85        33\n",
      "          04       0.95      0.86      0.90        44\n",
      "          05       1.00      0.50      0.67         4\n",
      "          06       0.90      0.92      0.91        39\n",
      "          07       0.40      0.40      0.40         5\n",
      "          08       0.94      0.94      0.94        47\n",
      "          09       0.77      0.86      0.81        28\n",
      "          10       0.90      0.96      0.93        48\n",
      "          11       0.89      0.89      0.89         9\n",
      "          12       0.91      0.91      0.91        47\n",
      "          13       0.84      0.95      0.89        43\n",
      "          14       0.91      0.93      0.92        45\n",
      "          15       1.00      0.67      0.80         3\n",
      "          16       0.93      0.87      0.90        15\n",
      "          17       0.81      0.97      0.89        36\n",
      "          18       0.86      0.88      0.87        41\n",
      "          19       0.94      0.84      0.89        19\n",
      "          20       0.97      0.92      0.94        37\n",
      "          21       0.90      0.92      0.91        48\n",
      "          22       0.98      0.89      0.93        47\n",
      "          23       0.89      1.00      0.94        17\n",
      "          24       0.94      0.94      0.94        31\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       0.98      0.96      0.97        49\n",
      "          27       0.84      1.00      0.91        51\n",
      "          28       1.00      0.83      0.91         6\n",
      "          29       1.00      0.82      0.90        11\n",
      "          30       0.60      0.60      0.60         5\n",
      "          31       0.91      0.96      0.93        51\n",
      "          32       0.90      0.90      0.90        21\n",
      "          33       0.98      0.94      0.96        47\n",
      "          34       1.00      0.50      0.67         2\n",
      "          36       1.00      0.96      0.98        24\n",
      "          38       1.00      0.82      0.90        11\n",
      "          40       1.00      0.50      0.67        10\n",
      "          42       0.90      0.92      0.91        50\n",
      "          44       0.91      0.95      0.93        22\n",
      "          46       0.95      1.00      0.98        40\n",
      "          48       0.98      0.89      0.93        47\n",
      "          50       0.91      0.93      0.92        44\n",
      "          52       0.89      0.87      0.88        38\n",
      "          54       0.83      0.67      0.74        15\n",
      "          56       1.00      0.20      0.33         5\n",
      "          58       0.67      0.40      0.50         5\n",
      "          60       0.98      0.91      0.95        47\n",
      "          78       0.89      0.94      0.91        34\n",
      "\n",
      "    accuracy                           0.91      1385\n",
      "   macro avg       0.91      0.84      0.86      1385\n",
      "weighted avg       0.91      0.91      0.91      1385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#KNN approach on HOG data\n",
    "#perform hyper-parameter-tuning and extract the most optimal model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "leaf_size = [1, 15, 20, 30]\n",
    "n_neighbors = [3, 5, 7, 10]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors)\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=3)\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train,y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "\n",
    "grid_predictions = clf.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, grid_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import mahotas\n",
    "import cv2\n",
    "#KNN approach on GLOBAL FEATURES data\n",
    "#perform hyper-parameter-tuning and extract the most optimal model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#on the most optimal model, train the KNN classifier on training data\n",
    "knn_2 = KNeighborsClassifier(leaf_size=1, n_neighbors=3)\n",
    "knnmodel = knn_2.fit(X_train,y_train)\n",
    "\n",
    "#test the model on training data\n",
    "y_pred= knnmodel.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090252707581228\n"
     ]
    }
   ],
   "source": [
    "#create a folder for saving models from JOBLIB\n",
    "import joblib\n",
    "os.chdir(r'C:\\Users\\Prerna Prakash Gupta\\Desktop\\savedmodel')\n",
    "filename = 'KNNGF1model.sav'\n",
    "joblib.dump(knnmodel, filename)\n",
    "# load the model from disk\n",
    "loaded_model = joblib.load('KNNGF1model.sav')\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "#we see 91% accuracy on test data\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 110, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "######perform hyper parameter optimization for RF and training training data on best performing model\n",
    "rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) \n",
    "\n",
    "param_grid = { 'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 3)\n",
    "CV_rfc.fit(X_train, y_train)\n",
    "print(CV_rfc.best_params_)\n",
    "\n",
    "grid_predictions = CV_rfc.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          01       1.00      0.94      0.97        33\n",
      "          02       1.00      0.97      0.98        29\n",
      "          03       0.97      0.94      0.95        33\n",
      "          04       0.94      1.00      0.97        44\n",
      "          05       1.00      0.50      0.67         4\n",
      "          06       1.00      0.95      0.97        39\n",
      "          07       1.00      0.20      0.33         5\n",
      "          08       1.00      0.96      0.98        47\n",
      "          09       1.00      0.89      0.94        28\n",
      "          10       0.94      0.98      0.96        48\n",
      "          11       1.00      1.00      1.00         9\n",
      "          12       0.98      0.96      0.97        47\n",
      "          13       0.98      0.98      0.98        43\n",
      "          14       1.00      0.93      0.97        45\n",
      "          15       1.00      0.33      0.50         3\n",
      "          16       1.00      0.87      0.93        15\n",
      "          17       0.92      0.97      0.95        36\n",
      "          18       0.77      0.98      0.86        41\n",
      "          19       1.00      0.89      0.94        19\n",
      "          20       0.90      0.95      0.92        37\n",
      "          21       0.98      0.94      0.96        48\n",
      "          22       0.98      0.91      0.95        47\n",
      "          23       0.89      1.00      0.94        17\n",
      "          24       0.97      0.94      0.95        31\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       0.87      0.98      0.92        49\n",
      "          27       0.87      0.92      0.90        51\n",
      "          28       1.00      1.00      1.00         6\n",
      "          29       0.82      0.82      0.82        11\n",
      "          30       1.00      0.80      0.89         5\n",
      "          31       0.98      0.98      0.98        51\n",
      "          32       0.95      1.00      0.98        21\n",
      "          33       0.88      0.94      0.91        47\n",
      "          34       1.00      1.00      1.00         2\n",
      "          36       1.00      0.96      0.98        24\n",
      "          38       0.82      0.82      0.82        11\n",
      "          40       1.00      0.70      0.82        10\n",
      "          42       1.00      0.94      0.97        50\n",
      "          44       1.00      0.95      0.98        22\n",
      "          46       0.95      1.00      0.98        40\n",
      "          48       0.94      1.00      0.97        47\n",
      "          50       0.93      0.93      0.93        44\n",
      "          52       0.95      0.92      0.93        38\n",
      "          54       0.83      0.67      0.74        15\n",
      "          56       0.62      1.00      0.77         5\n",
      "          58       1.00      0.40      0.57         5\n",
      "          60       1.00      0.96      0.98        47\n",
      "          78       0.81      1.00      0.89        34\n",
      "\n",
      "    accuracy                           0.94      1385\n",
      "   macro avg       0.95      0.89      0.90      1385\n",
      "weighted avg       0.95      0.94      0.94      1385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=1000, max_depth=110, min_samples_leaf=3, min_samples_split=8,    oob_score = True)\n",
    "rfc=rfc.fit(X_train, y_train)\n",
    "predict=rfc.predict(X_test) \n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9407942238267148\n"
     ]
    }
   ],
   "source": [
    "#create a folder for saving models from JOBLIB\n",
    "import joblib\n",
    "os.chdir(r'C:\\Users\\Prerna Prakash Gupta\\Desktop\\savedmodel')\n",
    "filename = 'RFCGLmodel1.sav'\n",
    "joblib.dump(rfc, filename)\n",
    "# load the model from disk\n",
    "loaded_model = joblib.load('RFCGLmodel1.sav')\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "#we see 94% accuracy on test data\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          01       1.00      0.91      0.95        33\n",
      "          02       0.97      1.00      0.98        29\n",
      "          03       0.89      0.94      0.91        33\n",
      "          04       0.98      0.98      0.98        44\n",
      "          05       1.00      0.75      0.86         4\n",
      "          06       1.00      0.97      0.99        39\n",
      "          07       0.67      0.40      0.50         5\n",
      "          08       0.96      0.96      0.96        47\n",
      "          09       0.93      0.89      0.91        28\n",
      "          10       0.94      0.98      0.96        48\n",
      "          11       1.00      1.00      1.00         9\n",
      "          12       0.96      0.96      0.96        47\n",
      "          13       0.89      0.95      0.92        43\n",
      "          14       1.00      0.93      0.97        45\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       1.00      0.87      0.93        15\n",
      "          17       1.00      1.00      1.00        36\n",
      "          18       0.95      0.93      0.94        41\n",
      "          19       1.00      0.89      0.94        19\n",
      "          20       0.95      0.97      0.96        37\n",
      "          21       0.94      0.96      0.95        48\n",
      "          22       0.96      0.94      0.95        47\n",
      "          23       0.94      1.00      0.97        17\n",
      "          24       0.94      0.94      0.94        31\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       0.98      0.96      0.97        49\n",
      "          27       0.98      0.94      0.96        51\n",
      "          28       0.71      0.83      0.77         6\n",
      "          29       0.75      0.82      0.78        11\n",
      "          30       0.83      1.00      0.91         5\n",
      "          31       0.96      0.96      0.96        51\n",
      "          32       1.00      0.90      0.95        21\n",
      "          33       0.96      0.98      0.97        47\n",
      "          34       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00        24\n",
      "          38       1.00      0.91      0.95        11\n",
      "          40       0.78      0.70      0.74        10\n",
      "          42       0.94      0.98      0.96        50\n",
      "          44       0.95      0.95      0.95        22\n",
      "          46       1.00      0.97      0.99        40\n",
      "          48       0.88      0.96      0.92        47\n",
      "          50       0.98      0.95      0.97        44\n",
      "          52       0.88      0.95      0.91        38\n",
      "          54       0.85      0.73      0.79        15\n",
      "          56       0.56      1.00      0.71         5\n",
      "          58       0.83      1.00      0.91         5\n",
      "          60       0.98      0.96      0.97        47\n",
      "          78       0.97      0.97      0.97        34\n",
      "\n",
      "    accuracy                           0.95      1385\n",
      "   macro avg       0.93      0.93      0.93      1385\n",
      "weighted avg       0.95      0.95      0.95      1385\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prerna Prakash Gupta\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#create an instance and fit the model \n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train, y_train)\n",
    "Predictions = logmodel.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_test,predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9458483754512635\n"
     ]
    }
   ],
   "source": [
    "#create a folder for saving models from JOBLIB\n",
    "import joblib\n",
    "os.chdir(r'C:\\Users\\Prerna Prakash Gupta\\Desktop\\savedmodel')\n",
    "filename = 'LOGGLmodel.sav'\n",
    "joblib.dump(logmodel, filename)\n",
    "# load the model from disk\n",
    "loaded_model = joblib.load('LOGGLmodel.sav')\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "#we see 95% accuracy on test data\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
